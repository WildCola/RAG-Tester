{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage generation\n",
    "\n",
    "Through this notebook, the vector store for each model will be generated and stored in the `storage` folder under the name of the model. \n",
    "Please note that this was developed thinking of only 2 models (Llama 2 and Mistral). This means that the `messages_to_prompt` function might need to be changed if you want to use it with other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rules and procedures\n",
    "data_path = os.path.join(parent_dir, 'data/qa')\n",
    "\n",
    "# #Legislation\n",
    "# data_path = os.path.join(parent_dir, 'data/law')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data ingestion\n",
    "\n",
    "documents = SimpleDirectoryReader(data_path, exclude_hidden=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Storing documents as a list to avoid loading them again\n",
    "with open('../storage/documents/documents.pickle'+data_path.split('/')[-1], 'wb') as f:\n",
    "    pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the stored documents\n",
    "with open('../storage/documents/documents.pickle'+data_path.split('/')[-1], 'rb') as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Activity R eport 20 22  \n",
      "DG ITEC , European Parliament  \n",
      "Effectiveness and efficiency of Directorate -General’s internal control system  27 \n",
      "o Operational set -up  \n",
      "o Document management.  \n",
      " five in 2021:  \n",
      "o Mission statement;  \n",
      "o Information and communications;  \n",
      "o Accou nting and financial infor-\n",
      "mation;  \n",
      "o Evaluation of activities;  \n",
      "o Audit reports.  \n",
      "At the end of  2021, DG ITEC completed its \n",
      "MICS review exercise. The control self -assess-\n",
      "ment (CSA) targeted the whole management of \n",
      "DG ITEC (Directors, Heads of Units and Heads \n",
      "of S ervices, excluding the Director -General) \n",
      "and a sample of staff without managerial roles, \n",
      "covering a total of 105 staff members. We ob-\n",
      "served a high participation rate (73%).  \n",
      "In 2022  the outcome of the survey was docu-\n",
      "mented in a specific report which was transmit-\n",
      "ted to DG ITEC’s management and discussed \n",
      "at ManagITEC  level (Director -General, Direc-\n",
      "tors and Heads of Units meeting).  Overall, the \n",
      "review confirmed the maturity of the Internal \n",
      "Control System. It resulted in recommenda-\n",
      "tions for improving the control environment of \n",
      "the Directorate -General, mainly by addressing \n",
      "the need for further awareness and training for \n",
      "newly appointed managers and newcomers \n",
      "without managerial role.  In addition, the selec-\n",
      "tion process for f our new managers was started \n",
      "in 2022, for w hom training  on internal control \n",
      "will be made available  in a continued effort to \n",
      "further raise awareness on internal control.  SENSITIVE FUNCTIONS EXERCISE  \n",
      "DG ITEC has run in 2022 a sensitive functions \n",
      "evaluation. This exercise followed the new \n",
      "methodology that identifies potentially sensitive \n",
      "functions on the basis of red flag indicators and \n",
      "an assessment of these potentially sensitive \n",
      "functions by a relevant hierarchical superior. \n",
      "The exercise ran a survey for ITEC \n",
      "management, in which not only the red fla g \n",
      "indicators were touched, but also \n",
      "implementation of mitigating controls.  Early \n",
      "indication shows generally a positive result, \n",
      "with potential improvements on the unique \n",
      "expertise and cybersecurity red flag categories \n",
      "- both of which will be strengthened i n control \n",
      "by the ongoing internalisation exercise, as well \n",
      "as the addition of information security posts \n",
      "planned in 2023 in the CISO directorate.  \n",
      "COMPLIANCE WITH EUDP R \n",
      "During the reporting period, DG ITEC’s Data \n",
      "Protection Coordinators:  \n",
      " provided a support  to the Institution’s Data \n",
      "Protection Officer (DPO), and to DG ITEC’s \n",
      "top management and data controllers, with \n",
      "a total of 127 consultations closed;  \n",
      " ensured a compliance with the Regulation \n",
      "(EU) 2018/1725 (EUDPR) with a direct in-\n",
      "volvement in the follow up of ICT projects, \n",
      "and the acquisition of software and ICT in-\n",
      "frastruc ture.  \n",
      "Overall, DG ITEC has significantly improved \n",
      "the maturity of its processes related to the \n",
      "protection of personal data, increasing further \n",
      "its compliance with the EU Data Protection \n",
      "Regulation. The creation of a new \n",
      "organisational Unit will allow an increase \n"
     ]
    }
   ],
   "source": [
    "print(documents[100].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/splacintescu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import NLTKTextSplitter\n",
    "# text_splitter = NLTKTextSplitter()\n",
    "\n",
    "#better results with SpaCy\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter()\n",
    "\n",
    "# for i in range(len(documents)):\n",
    "#     documents[i].text = ''.join(text_splitter.split_text((documents[i].text)))\n",
    "\n",
    "chunkedText = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text((doc.text))\n",
    "    for chunk in chunks:\n",
    "        doc_aux = copy.deepcopy(doc)\n",
    "        doc_aux.text = chunk\n",
    "        chunkedText.append(doc_aux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Activity R eport 20 22  \n",
      "DG ITEC , European Parliament  \n",
      "Effectiveness and efficiency of Directorate -General’s internal control system  \n",
      "\n",
      "27 \n",
      "o Operational set -up  \n",
      "o Document management.  \n",
      "\n",
      "\n",
      " five in 2021:  \n",
      "o Mission statement;  \n",
      "o Information and communications;  \n",
      "o\n",
      "\n",
      "Accou nting and financial infor-\n",
      "mation;  \n",
      "o Evaluation of activities;  \n",
      "\n",
      "\n",
      "o Audit reports.  \n",
      "\n",
      "\n",
      "At the end of  2021, DG ITEC completed its \n",
      "MICS review exercise.\n",
      "\n",
      "The control self -assess-\n",
      "ment (CSA) targeted the whole management of \n",
      "DG ITEC (Directors, Heads of Units and Heads \n",
      "of S ervices, excluding the Director -General) \n",
      "and a sample of staff without managerial roles, \n",
      "covering a total of 105 staff members.\n",
      "\n",
      "We ob-\n",
      "served a high participation rate (73%).  \n",
      "\n",
      "\n",
      "In 2022  the outcome of the survey was docu-\n",
      "mented in a specific report which was transmit-\n",
      "ted to DG ITEC’s management and discussed \n",
      "at ManagITEC  level (Director -General, Direc-\n",
      "tors and Heads of Units meeting).  \n",
      "\n",
      "Overall, the \n",
      "review confirmed the maturity of the Internal \n",
      "Control System.\n",
      "\n",
      "It resulted in recommenda-\n",
      "tions for improving the control environment of \n",
      "the Directorate -General, mainly by addressing \n",
      "the need for further awareness and training for \n",
      "newly appointed managers and newcomers \n",
      "without managerial role.  \n",
      "\n",
      "In addition, the selec-\n",
      "tion process for f our new managers was started \n",
      "in 2022, for w hom training  on internal control \n",
      "will be made available  in a continued effort to \n",
      "further raise awareness on internal control.  \n",
      "\n",
      "SENSITIVE FUNCTIONS EXERCISE  \n",
      "DG ITEC has run in 2022 a sensitive functions \n",
      "evaluation.\n",
      "\n",
      "This exercise followed the new \n",
      "methodology that identifies potentially sensitive \n",
      "functions on the basis of red flag indicators and \n",
      "an assessment of these potentially sensitive \n",
      "functions by a relevant hierarchical superior. \n",
      "\n",
      "\n",
      "The exercise ran a survey for ITEC \n",
      "management, in which not only the red fla g \n",
      "indicators were touched, but also \n",
      "implementation of mitigating controls.  \n",
      "\n",
      "Early \n",
      "indication shows generally a positive result, \n",
      "with potential improvements on the unique \n",
      "expertise and cybersecurity red flag categories \n",
      "- both of which will be strengthened i n control \n",
      "by the ongoing internalisation exercise, as well \n",
      "as the addition of information security posts \n",
      "planned in 2023 in the CISO directorate.  \n",
      "\n",
      "\n",
      "COMPLIANCE WITH EUDP R \n",
      "During the reporting period, DG ITEC’s Data \n",
      "Protection Coordinators:  \n",
      " provided a support  to the Institution’s Data \n",
      "Protection Officer (DPO), and to DG ITEC’s \n",
      "top management and data controllers, with \n",
      "a total of 127 consultations closed;  \n",
      " ensured a compliance with the Regulation \n",
      "(EU) 2018/1725 (EUDPR) with a direct in-\n",
      "volvement in the follow up of ICT projects, \n",
      "and the acquisition of software and ICT in-\n",
      "frastruc ture.  \n",
      "\n",
      "\n",
      "Overall, DG ITEC has significantly improved \n",
      "the maturity of its processes related to the \n",
      "protection of personal data, increasing further \n",
      "its compliance with the EU Data Protection \n",
      "Regulation.\n",
      "\n",
      "The creation of a new \n",
      "organisational Unit will allow an increase\n"
     ]
    }
   ],
   "source": [
    "print(chunkedText[100].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a model to generate storage\n",
    "This could be combined with a for loop, but to avoid memory issues, we run it separately for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: ['mixtral-8x7b-instruct-v0', '1', 'Q8_0']\n",
      "1: ['llama-2-13b-chat', 'Q4_0']\n",
      "2: ['mistral-7b-instruct-v0', '2', 'Q5_K_M']\n",
      "3: ['mixtral-8x7b-instruct-v0', '1', 'Q3_K_M']\n"
     ]
    }
   ],
   "source": [
    "# Construct the path to the models directory\n",
    "models_path = os.path.join(parent_dir, 'models')\n",
    "models = [f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f))]\n",
    "try:\n",
    "    # remove .gitignore by specifying the name\n",
    "    models.remove(\".gitignore\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # remove anything ending with Zone.Identifier\n",
    "    models = [m for m in models if not m.endswith(\"Zone.Identifier\")]\n",
    "except:\n",
    "    pass\n",
    "# From every entry, remove everything after the first dot\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m.split('.')[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Please enter a number between 0 and 3 according to the selection shown above.\n"
     ]
    }
   ],
   "source": [
    "# Select a model. The user can only input a number between 0 and len(models)-1, if he inputs something else, the program will ask again\n",
    "while True:\n",
    "    try:\n",
    "        model_index = int(input(\"Select a model: \"))\n",
    "        if model_index >= 0 and model_index < len(models):\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a number between 0 and \" + str(len(models)-1) + \" according to the selection shown above.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number between 0 and \" + str(len(models)-1) + \" according to the selection shown above.\")\n",
    "\n",
    "# Get path to the selected model\n",
    "model_path = os.path.join(models_path, models[model_index])\n",
    "model_tag = models[model_index].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /home/splacintescu/RAG-Tester/models/mixtral-8x7b-instruct-v0.1.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 46.22 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors:        CPU buffer size = 47324.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   303.40 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1668\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "if not models[model_index].startswith(\"llama\"):\n",
    "        # The following prompt works well with Mistral\n",
    "        def messages_to_prompt(messages):\n",
    "                prompt = \"\"\n",
    "                for message in messages:\n",
    "                        if message.role == 'system':\n",
    "                                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "                        elif message.role == 'user':\n",
    "                                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "                        elif message.role == 'assistant':\n",
    "                                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "                        # ensure we start with a system prompt, insert blank if needed\n",
    "                        if not prompt.startswith(\"<|system|>\\n\"):\n",
    "                                prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "                        # add final assistant prompt\n",
    "                        prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "                return prompt\n",
    "\n",
    "llm = LlamaCPP(\n",
    "        # You can pass in the URL to a GGML model to download it automatically\n",
    "        # model_url=model_url,\n",
    "        # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "        model_path=model_path,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1000,\n",
    "        # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "        context_window=3900,\n",
    "        # kwargs to pass to __call__()\n",
    "        generate_kwargs={},\n",
    "        # kwargs to pass to __init__()\n",
    "        # set to at least 1 to use GPU\n",
    "        model_kwargs={\"n_gpu_layers\": -1},\n",
    "        # transform inputs into Llama2 format\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Embeddings model\n",
    "Currently model name needs to be changed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 'sentence-transformers/all-mpnet-base-v2' # 'intfloat/e5-large-v2' #  \"BAAI/bge-large-en-v1.5\"   # \"BAAI/bge-base-en-v1.5\"\n",
    "embedding_tag = embedding.split('/')[1]\n",
    "embed_model = HuggingFaceEmbedding(embedding, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model= embed_model,\n",
    "    # \"local:EuropeanParliament/eubert_embedding_v1\",    \n",
    "    chunk_size=512,\n",
    "    chunk_overlap=125,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 7248/7248 [00:11<00:00, 634.64it/s] \n",
      "Generating embeddings: 100%|██████████| 12045/12045 [03:16<00:00, 61.23it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(chunkedText, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir=f\"../storage/{embedding_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading index\n",
    "Uncomment the following cell if you want to load an index from a previous run and test the storage loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rebuild storage context\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=f\"../storage/{embedding_tag}\")\n",
    "\n",
    "# # load index\n",
    "# vector_index = load_index_from_storage(storage_context, service_context= service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new documents to existing index\n",
    "If new documents want to be added, then follow the following steps (**LOAD INDEX AND SERVICE CONTEXT FIRST**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.join(parent_dir, 'data', 'EUWhoiswho_EP_EN.pdf')\n",
    "\n",
    "# # Data ingestion\n",
    "# new_documents = SimpleDirectoryReader(input_files=[data_path]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add to index\n",
    "# for chunk in new_documents:\n",
    "#     vector_index.insert(chunk, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist to disk\n",
    "# vector_index.storage_context.persist(persist_dir=f\"../storage/{embedding_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to update the document store in case it is needed in the future! (Loading documents section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
