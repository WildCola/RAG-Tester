{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage generation\n",
    "\n",
    "Through this notebook, the vector store for each model will be generated and stored in the `storage` folder under the name of the model. \n",
    "Please note that this was developed thinking of only 2 models (Llama 2 and Mistral). This means that the `messages_to_prompt` function might need to be changed if you want to use it with other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rules and procedures\n",
    "data_path = os.path.join(parent_dir, 'data/rp')\n",
    "\n",
    "# #Legislation\n",
    "# data_path = os.path.join(parent_dir, 'data/law')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion\n",
    "\n",
    "documents = SimpleDirectoryReader(data_path, exclude_hidden=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing documents as a list to avoid loading them again\n",
    "with open('../storage/documents/documents.pickle'+data_path.split('/')[-1], 'wb') as f:\n",
    "    pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the stored documents\n",
    "with open('../storage/documents/documents.pickle'+data_path.split('/')[-1], 'rb') as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE649.486v01-00Question for written answer E-001354/2020\n",
      "to the Commission\n",
      "Rule 138\n",
      "Eugen Tomac (PPE)\n",
      "Subject: 2021-2027 common agricultural policy\n",
      "The common agricultural policy has clearly benefited Romania, resulting in a more competitive \n",
      "farming sector, the more effective use of natural resources and a dramatic rise in living standards for \n",
      "rural areas.\n",
      "How will the Commission ensure that the new common agricultural policy for 2021-2027 properly \n",
      "reflects Romania's priorities and needs?\n",
      "More specifically:\n",
      "What funding has been allocated to Romania for 2021-2027 compared with 2014-2020?\n",
      "What are the subsidy levels per hectare/crop in 2021, compared to 2020?\n",
      "What are the subsidy levels for beef/pork/ poultry/sheep 2021, compared to 2020?\n",
      "EN\n",
      "E-0001354/2020\n",
      "Answer given by Mr Wojciechowski\n",
      "on behalf of the European Commission\n",
      "(27.4.2020)\n",
      "The Common Agricultural Policy (CAP) proposed for the period post 2020 will give \n",
      "Romania greater responsibility and possibilities to tailor their policy measures to their \n",
      "national and local conditions and needs, within the allocated funding. \n",
      "The proposal for the multiannual financial framework (MFF) for the period 2021-20271 and \n",
      "for the CAP post 20202 allocate EUR 20.5 billion (in current prices) to Romania. This \n",
      "represents a moderate reduction of 5.3% compared to the baseline allocation3.\n",
      "The allocation for Romania for the period 2014-2020 amounts to EUR 20.1 billion, which \n",
      "means that the proposed allocation for 2021-2027 actually increases. However, a comparison \n",
      "to the 2014-2020 period gives misleading information as the figures for 2014-2020 include \n",
      "the:\n",
      "?effect of the external convergence of the direct payments over the period 2014-2020, \n",
      "whereby Romania?s allocation gradually increases as agreed in 2013, and\n",
      "?effect of ?phasing in? which gradually increases the direct payments in Romania.\n",
      "To assess the sole effect of the proposals for 2021-2027 (and not the effect of the changes \n"
     ]
    }
   ],
   "source": [
    "print(documents[100].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/splacintescu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import NLTKTextSplitter\n",
    "# text_splitter = NLTKTextSplitter()\n",
    "\n",
    "#better results with SpaCy\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter()\n",
    "\n",
    "# for i in range(len(documents)):\n",
    "#     documents[i].text = ''.join(text_splitter.split_text((documents[i].text)))\n",
    "\n",
    "chunkedText = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text((doc.text))\n",
    "    for chunk in chunks:\n",
    "        doc_aux = copy.deepcopy(doc)\n",
    "        doc_aux.text = chunk\n",
    "        chunkedText.append(doc_aux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE649.486v01-00Question for written answer E-001354/2020\n",
      "to the Commission\n",
      "Rule 138\n",
      "Eugen Tomac (PPE)\n",
      "\n",
      "\n",
      "Subject: 2021-2027 common agricultural policy\n",
      "The common agricultural policy has clearly benefited Romania, resulting in a more competitive \n",
      "farming sector, the more effective use of natural resources and a dramatic rise in living standards for \n",
      "rural areas.\n",
      "\n",
      "\n",
      "How will the Commission ensure that the new common agricultural policy for 2021-2027 properly \n",
      "reflects Romania's priorities and needs?\n",
      "More specifically:\n",
      "What funding has been allocated to Romania for 2021-2027 compared with 2014-2020?\n",
      "\n",
      "\n",
      "What are the subsidy levels per hectare/crop in 2021, compared to 2020?\n",
      "\n",
      "\n",
      "What are the subsidy levels for beef/pork/ poultry/sheep 2021, compared to 2020?\n",
      "EN\n",
      "E-0001354/2020\n",
      "Answer given by Mr Wojciechowski\n",
      "on behalf of the European Commission\n",
      "(27.4.2020)\n",
      "\n",
      "\n",
      "The Common Agricultural Policy (CAP) proposed for the period post 2020 will give \n",
      "Romania greater responsibility and possibilities to tailor their policy measures to their \n",
      "national and local conditions and needs, within the allocated funding. \n",
      "\n",
      "\n",
      "The proposal for the multiannual financial framework (MFF) for the period 2021-20271 and \n",
      "for the CAP post 20202 allocate EUR 20.5 billion (in current prices) to Romania.\n",
      "\n",
      "This \n",
      "represents a moderate reduction of 5.3% compared to the baseline allocation3.\n",
      "\n",
      "\n",
      "The allocation for Romania for the period 2014-2020 amounts to EUR 20.1 billion, which \n",
      "means that the proposed allocation for 2021-2027 actually increases.\n",
      "\n",
      "However, a comparison \n",
      "to the 2014-2020 period gives misleading information as the figures for 2014-2020 include \n",
      "the:\n",
      "?effect of the external convergence of the direct payments over the period 2014-2020, \n",
      "whereby Romania?s allocation gradually increases as agreed in 2013, and\n",
      "?effect of ?phasing in? which gradually increases the direct payments in Romania.\n",
      "\n",
      "\n",
      "To assess the sole effect of the proposals for 2021-2027 (and not the effect of the changes\n"
     ]
    }
   ],
   "source": [
    "print(chunkedText[100].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a model to generate storage\n",
    "This could be combined with a for loop, but to avoid memory issues, we run it separately for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: ['mixtral-8x7b-instruct-v0', '1', 'Q8_0']\n",
      "1: ['llama-2-13b-chat', 'Q4_0']\n",
      "2: ['mistral-7b-instruct-v0', '2', 'Q5_K_M']\n",
      "3: ['mixtral-8x7b-instruct-v0', '1', 'Q3_K_M']\n"
     ]
    }
   ],
   "source": [
    "# Construct the path to the models directory\n",
    "models_path = os.path.join(parent_dir, 'models')\n",
    "models = [f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f))]\n",
    "try:\n",
    "    # remove .gitignore by specifying the name\n",
    "    models.remove(\".gitignore\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # remove anything ending with Zone.Identifier\n",
    "    models = [m for m in models if not m.endswith(\"Zone.Identifier\")]\n",
    "except:\n",
    "    pass\n",
    "# From every entry, remove everything after the first dot\n",
    "print(\"Available models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(f\"{i}: {m.split('.')[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model. The user can only input a number between 0 and len(models)-1, if he inputs something else, the program will ask again\n",
    "while True:\n",
    "    try:\n",
    "        model_index = int(input(\"Select a model: \"))\n",
    "        if model_index >= 0 and model_index < len(models):\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a number between 0 and \" + str(len(models)-1) + \" according to the selection shown above.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number between 0 and \" + str(len(models)-1) + \" according to the selection shown above.\")\n",
    "\n",
    "# Get path to the selected model\n",
    "model_path = os.path.join(models_path, models[model_index])\n",
    "model_tag = models[model_index].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from /home/splacintescu/RAG-Tester/models/mixtral-8x7b-instruct-v0.1.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 46.22 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors:        CPU buffer size = 47324.64 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   303.40 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1668\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "if not models[model_index].startswith(\"llama\"):\n",
    "        # The following prompt works well with Mistral\n",
    "        def messages_to_prompt(messages):\n",
    "                prompt = \"\"\n",
    "                for message in messages:\n",
    "                        if message.role == 'system':\n",
    "                                prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "                        elif message.role == 'user':\n",
    "                                prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "                        elif message.role == 'assistant':\n",
    "                                prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "                        # ensure we start with a system prompt, insert blank if needed\n",
    "                        if not prompt.startswith(\"<|system|>\\n\"):\n",
    "                                prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "                        # add final assistant prompt\n",
    "                        prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "                return prompt\n",
    "\n",
    "llm = LlamaCPP(\n",
    "        # You can pass in the URL to a GGML model to download it automatically\n",
    "        # model_url=model_url,\n",
    "        # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "        model_path=model_path,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1000,\n",
    "        # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "        context_window=3900,\n",
    "        # kwargs to pass to __call__()\n",
    "        generate_kwargs={},\n",
    "        # kwargs to pass to __init__()\n",
    "        # set to at least 1 to use GPU\n",
    "        model_kwargs={\"n_gpu_layers\": -1},\n",
    "        # transform inputs into Llama2 format\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Embeddings model\n",
    "Currently model name needs to be changed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 'sentence-transformers/all-mpnet-base-v2' # 'intfloat/e5-large-v2' #  \"BAAI/bge-large-en-v1.5\"   # \"BAAI/bge-base-en-v1.5\"\n",
    "embedding_tag = embedding.split('/')[1]\n",
    "embed_model = HuggingFaceEmbedding(embedding, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model= embed_model,\n",
    "    # \"local:EuropeanParliament/eubert_embedding_v1\",    \n",
    "    chunk_size=512,\n",
    "    chunk_overlap=125,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/43625 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 43625/43625 [00:39<00:00, 1093.75it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:24<00:00, 83.80it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.63it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.60it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:24<00:00, 83.48it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:30<00:00, 67.28it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:33<00:00, 61.80it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 59.17it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:30<00:00, 66.76it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:35<00:00, 58.45it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.73it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 59.07it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.61it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 59.02it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.85it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.96it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.67it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 59.13it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:36<00:00, 56.13it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:35<00:00, 57.76it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:35<00:00, 58.31it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:33<00:00, 61.48it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:23<00:00, 88.65it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:34<00:00, 58.91it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:32<00:00, 62.64it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:26<00:00, 77.56it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:22<00:00, 90.44it/s] \n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:21<00:00, 96.96it/s] \n",
      "Generating embeddings: 100%|██████████| 1489/1489 [00:15<00:00, 99.02it/s] \n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(chunkedText, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir=f\"../storage/{embedding_tag}-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading index\n",
    "Uncomment the following cell if you want to load an index from a previous run and test the storage loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rebuild storage context\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=f\"../storage/{embedding_tag}\")\n",
    "\n",
    "# # load index\n",
    "# vector_index = load_index_from_storage(storage_context, service_context= service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new documents to existing index\n",
    "If new documents want to be added, then follow the following steps (**LOAD INDEX AND SERVICE CONTEXT FIRST**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.join(parent_dir, 'data', 'EUWhoiswho_EP_EN.pdf')\n",
    "\n",
    "# # Data ingestion\n",
    "# new_documents = SimpleDirectoryReader(input_files=[data_path]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add to index\n",
    "# for chunk in new_documents:\n",
    "#     vector_index.insert(chunk, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist to disk\n",
    "# vector_index.storage_context.persist(persist_dir=f\"../storage/{embedding_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to update the document store in case it is needed in the future! (Loading documents section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
